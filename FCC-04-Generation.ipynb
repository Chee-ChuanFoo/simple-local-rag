{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Pre-processing\n",
    "import fitz # install PyMuPDF\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.en import English \n",
    "import re\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "# RAG\n",
    "import torch\n",
    "import textwrap\n",
    "from time import perf_counter as timer\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# HuggingFace token\n",
    "access_token = os.environ['hugging_face_api_key']\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File human-nutrition-text.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# Get PDF document path\n",
    "filename = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF\n",
    "if not os.path.exists(filename):\n",
    "    print(\"File doesn't exist, downloading...\")\n",
    "    \n",
    "    # The URL of the PDF you want to download\n",
    "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "    # Send a GET request to the url\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open the file and save it\n",
    "        with open(filename,\"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"[INFO] The file has been download and saved as {filename}\")\n",
    "    else: print(f\"[INFO] Failed to download the file. Status Code: {response.status_code}\")\n",
    "        \n",
    "else:\n",
    "  print(f\"File {filename} exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format text read from PDF\n",
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\",\" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Provide pdf name\n",
    "def open_and_read_pdf(filename: str)->list[dict]:\n",
    "    doc = fitz.open(filename)\n",
    "    pages_and_texts=[]\n",
    "    for page_number,page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        formatted_text = text_formatter(text)\n",
    "\n",
    "        # store pages information and texts in a dictionary\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                        \"page_char_count\": len(text),\n",
    "                        \"page_word_count\": len(text.split(\" \")),\n",
    "                        \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                        # \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                        \"text\": text})\n",
    "        \n",
    "    return pages_and_texts\n",
    "\n",
    "def read_pages(pages,page_number:int):\n",
    "    # insert pages_and_text and page number to retrieve the specified page in the pdf\n",
    "    content  = pages[page_number+41]\n",
    "    return content\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(pages_and_texts,num_sentence_chunk_size=10,min_token_length=30):\n",
    "    nlp = English()\n",
    "    # Add a sentencizer pipeline\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    # Sentenciaing texts on each page to sentences\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "        \n",
    "        # Make sure all sentences are strings\n",
    "        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "        \n",
    "        # Count the sentences \n",
    "        item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n",
    "    # Loop through pages and texts and split sentences into chunks\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                            slice_size=num_sentence_chunk_size)\n",
    "        item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "\n",
    "    # Split each chunk into its own item\n",
    "    pages_and_chunks = []\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "            \n",
    "            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "            \n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "    df = pd.DataFrame(pages_and_chunks)\n",
    "    pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "    return pages_and_chunks, pages_and_chunks_over_min_token_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(texts,model_name=\"all-mpnet-base-v2\",device=\"cuda\"):\n",
    "\n",
    "    # use \"all-mpnet-base-v2\"\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=model_name, \n",
    "                                        device=device)\n",
    "    # # Send the model to the GPU\n",
    "    # embedding_model.to(\"cuda\") \n",
    "\n",
    "    # Create embeddings one by one on the GPU\n",
    "    # pages_and_chunks_over_min_token_len\n",
    "    for item in tqdm(texts):\n",
    "        item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "\n",
    "    # # Turn text chunks into a single list\n",
    "    # text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "\n",
    "    # # Embed all texts in batches (z-book: 34.4s)\n",
    "    # text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "    #                                                batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "    #                                                convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "    # Save embeddings to file\n",
    "    text_chunks_and_embeddings_df = pd.DataFrame(texts)\n",
    "    embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "    text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)\n",
    "    return text_chunks_and_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_from_file(file='text_chunks_and_embeddings_df.csv',device='cuda'):\n",
    "    # Import texts and embedding df\n",
    "    text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "    # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "    # Convert texts and embedding df to list of dicts\n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "    embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "    return pages_and_chunks,embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [00:00, 1284.06it/s]\n",
      "100%|██████████| 1208/1208 [00:01<00:00, 717.73it/s]\n",
      "100%|██████████| 1208/1208 [00:00<00:00, 604115.80it/s]\n",
      "100%|██████████| 1208/1208 [00:00<00:00, 37549.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'page_number': 196,\n",
       " 'sentence_chunk': 'Potassium also is involved in protein synthesis, energy metabolism, \\nand platelet function, and acts as a buffer in blood, playing a role in \\nacid-base balance.\\nImbalances of Potassium \\nInsufficient potassium levels in the body (hypokalemia) can be \\ncaused by a low dietary intake of potassium or by high sodium \\nintakes, but more commonly it results from medications that \\nincrease water excretion, mainly diuretics. The signs and symptoms \\nof hypokalemia are related to the functions of potassium in nerve \\ncells and consequently skeletal and smooth-muscle contraction.\\nThe signs and symptoms include muscle weakness and cramps, \\nrespiratory distress, and constipation. Severe potassium depletion \\ncan cause the heart to have abnormal contractions and can even \\nbe fatal. High levels of potassium in the blood, or hyperkalemia, \\nalso affects the heart. It is a silent condition as it often displays \\nno signs or symptoms. Extremely high levels of potassium in the \\nblood disrupt the electrical impulses that stimulate the heart and \\ncan cause the heart to stop. Hyperkalemia is usually the result of \\nkidney dysfunction.\\nNeeds and Dietary Sources of Potassium \\nThe IOM based their AIs for potassium on the levels associated with \\na decrease in blood pressure, a reduction in salt sensitivity, and a \\nminimal risk of kidney stones.',\n",
       " 'chunk_char_count': 1332,\n",
       " 'chunk_word_count': 203,\n",
       " 'chunk_token_count': 333.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing\n",
    "pages_and_texts = open_and_read_pdf(\"human-nutrition-text.pdf\")\n",
    "pages_and_chunks,pages_and_chunks_over_min_token_len = text_processor(pages_and_texts)\n",
    "read_pages(pages_and_chunks_over_min_token_len,258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embedding from csv file\n",
    "pages_and_chunks,embeddings = read_embedding_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = torch.dot(vector1, vector2)\n",
    "\n",
    "    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Pass user query into the function,\n",
    "    Embeds a query with model and returns top k (default=5) scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores based on embeddings passed into the function\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    \"\"\"\n",
    "    Return top 5 results based on dot product scores    \n",
    "    \"\"\"\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking local GPU memory availability\n",
    "\n",
    "Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n",
    "\n",
    "> **Note:** You can also check this with the `!nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 10 GB\n"
     ]
    }
   ],
   "source": [
    "    # Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 10 | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\n",
      "use_quantization_config set to: False\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an LLM locally\n",
    "\n",
    "Alright! Looks like `gemma-7b-it` it is (for my local machine with an RTX 4090, change the `model_id` and `use_quantization_config` values to suit your needs)! \n",
    "\n",
    "There are plenty of examples of how to load the model on the `gemma-7b-it` [Hugging Face model card](https://huggingface.co/google/gemma-7b-it).\n",
    "\n",
    "Good news is, the Hugging Face [`transformers`](https://huggingface.co/docs/transformers/) library has all the tools we need.\n",
    "\n",
    "To load our LLM, we're going to need a few things:\n",
    "1. A quantization config (optional) - This will determine whether or not we load the model in 4bit precision for lower memory usage. The we can create this with the [`transformers.BitsAndBytesConfig`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/quantization#transformers.BitsAndBytesConfig) class (requires installing the [`bitsandbytes` library](https://github.com/TimDettmers/bitsandbytes)).\n",
    "2. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example `gemma-7b-it`.\n",
    "3. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the [`transformers.AutoTokenzier.from_pretrained`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) method and passing it our model ID.\n",
    "4. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the [`transformers.AutoModelForCausalLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method and passing it our model ID as well as other various parameters.\n",
    "\n",
    "As a bonus, we'll check if [Flash Attention 2](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2) is available using `transformers.utils.is_flash_attn_2_available()`. Flash Attention 2 speeds up the attention mechanism in Transformer architecture models (which is what many modern LLMs are based on, including Gemma). So if it's available and the model is supported (not all models support Flash Attention 2), we'll use it. If it's not available, you can install it by following the instructions on the [GitHub repo](https://github.com/Dao-AILab/flash-attention). \n",
    "\n",
    "> **Note:** Flash Attention 2 currently works on NVIDIA GPUs with a compute capability score of 8.0+ (Ampere, Ada Lovelace, Hopper architectures). We can check our GPU compute capability score with [`torch.cuda.get_device_capability(0)`](https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html). \n",
    "\n",
    "> **Note:** To get access to the Gemma models, you will have to [agree to the terms & conditions](https://huggingface.co/google/gemma-7b-it) on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the [Hugging Face CLI/Hugging Face Hub `login()` function](https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication). Once you've done this, you'll be able to download the models. If you're using Google Colab, you can add a [Hugging Face token](https://huggingface.co/docs/hub/en/security-tokens) to the \"Secrets\" tab.\n",
    ">\n",
    "> Downloading an LLM locally can take a fair bit of time depending on your internet connection. Gemma 7B is about a 16GB download and Gemma 2B is about a 6GB download.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20d803fabe1454b8b1eccc8eed10fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Create quantization config for smaller model loading (optional)\n",
    "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
    "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
    "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
    "#model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id # (we already set this above)\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id, token=access_token)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use full memory \n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU \n",
    "    llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506172416"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 5079453696, 'model_mem_mb': 4844.14, 'model_mem_gb': 4.73}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the memory requirements\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the scaffolding around our input text, this is the kind of turn-by-turn instruction tuning our model has gone through.\n",
    "\n",
    "Our next step is to tokenize this formatted text and pass it to our model's `generate()` method.\n",
    "\n",
    "We'll make sure our tokenized text is on the same device as our model (GPU) using `to(\"cuda\")`.\n",
    "\n",
    "Let's generate some text! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   1841,    708,    573, 186809,\n",
      "         184592, 235269,    578,   1212,  16065,    749,    984,   1554,    575,\n",
      "            573,   3515,   2971, 235336,    107,    108,    106,   2516,    108]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cheefoo3\\OneDrive\\Code\\Python\\DeepLearning.AI\\14 local-RAG-Daniel_Bourke\\simple-local-rag\\venv\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (tokens):\n",
      "tensor([     2,      2,    106,   1645,    108,   1841,    708,    573, 186809,\n",
      "        184592, 235269,    578,   1212,  16065,    749,    984,   1554,    575,\n",
      "           573,   3515,   2971, 235336,    107,    108,    106,   2516,    108,\n",
      "         21404, 235269,   1517, 235303, 235256,    476,  25497,    576,    573,\n",
      "        186809, 184592,    578,   1024,  16065,    575,    573,   3515,   2971,\n",
      "        235292,    109,    688,  12298,   1695, 184592,  66058,    109, 235287,\n",
      "          5231, 156615,  56227,  66058,    108,    141, 235287,  34428,   4134,\n",
      "           604,    573,   2971, 235303, 235256,   5999,    578,  29703, 235265,\n",
      "           108,    141, 235287, 110165,  56227,    708,    573,   7920,   4303,\n",
      "           576,   4134,    604,   1546,   5999, 235265,    108,    141, 235287,\n",
      "         25280,  72780,    708,   1941,    674,   1987,   5543,    577,  55997,\n",
      "        235269,   1582,    685,   3733,  29907, 235269,  16803, 235269,    578,\n",
      "         19574, 235265,    108,    141, 235287,  13702,  72780,    708,   1941,\n",
      "           674,    708,   7290, 122712, 235269,   1582,    685,   9347, 235269,\n",
      "         57634, 235269,    578, 109955, 235265,    109, 235287,   5231, 216954,\n",
      "         66058,    108,    141, 235287,   8108,    578,  12158,  29703, 235269,\n",
      "         44760, 235269,    578,  53186, 235265,    108,    141, 235287,  96084,\n",
      "           708,   8727,    604,  24091,   1411, 235269,  42696,   4584, 235269,\n",
      "           578,  14976,  12158, 235265,    108,    141, 235287,   2456,    708,\n",
      "          2167,   5088,    576,  20361, 235269,   1853,    675,   3724,   7257,\n",
      "        235265,    109, 235287,   5231, 235311,   1989,  66058,    108,    141,\n",
      "        235287,  34428,   4134, 235269,  38823, 235269,    578,   1707,  33398,\n",
      "         48765, 235265,    108,    141, 235287,  41137,  61926,   3707,  21361,\n",
      "          4684, 235269,  59269, 235269,  22606, 235269,    578,  15741, 235265,\n",
      "           108,    141, 235287,   3776,  61926,    798,  12310,  45365,   5902,\n",
      "           578,   4740,    573,   5685,    576,   3760,   7197, 235265,    109,\n",
      "           688,  33771,    576,  97586, 184592,    575,    573,   9998,  14427,\n",
      "         66058,    109, 235287,   5231,  23920,   4584,  66058, 110165,  56227,\n",
      "        235269,  20361, 235269,    578,  61926,   3658,    573,   2971,    675,\n",
      "          4134, 235265,    108, 235287,   5231,  25251,    578,  68808,  29703,\n",
      "         66058,  96084,    708,   8727,    604,   4547,    578,  68808,  29703,\n",
      "        235269,   3359,  22488, 235269], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig \n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! We just generated some text on our local GPU!\n",
    "\n",
    "Well not just yet...\n",
    "\n",
    "Our LLM accepts tokens in and sends tokens back out.\n",
    "\n",
    "We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here's a breakdown of the macronutrients and their roles in the human body:\n",
      "\n",
      "**Macronutrients:**\n",
      "\n",
      "* **Carbohydrates:**\n",
      "    * Provide energy for the body's cells and tissues.\n",
      "    * Carbohydrates are the primary source of energy for most cells.\n",
      "    * Complex carbohydrates are those that take longer to digest, such as whole grains, fruits, and vegetables.\n",
      "    * Simple carbohydrates are those that are quickly digested, such as sugar, starch, and lactose.\n",
      "\n",
      "* **Proteins:**\n",
      "    * Build and repair tissues, enzymes, and hormones.\n",
      "    * Proteins are essential for immune function, hormone production, and tissue repair.\n",
      "    * There are different types of proteins, each with specific functions.\n",
      "\n",
      "* **Fats:**\n",
      "    * Provide energy, insulation, and help absorb vitamins.\n",
      "    * Healthy fats include olive oil, avocado, nuts, and seeds.\n",
      "    * Trans fats can raise cholesterol levels and increase the risk of heart disease.\n",
      "\n",
      "**Roles of Macronutrients in the Human Body:**\n",
      "\n",
      "* **Energy production:** Carbohydrates, proteins, and fats provide the body with energy.\n",
      "* **Building and repairing tissues:** Proteins are essential for building and repairing tissues, including muscles,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That looks like a pretty good answer.\n",
    "\n",
    "But notice how the output contains the prompt text as well?\n",
    "\n",
    "How about we do a little formatting to replace the prompt in the output text?\n",
    "\n",
    "> **Note:** `\"<bos>\"` and `\"<eos>\"` are special tokens to denote \"beginning of sentence\" and \"end of sentence\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Output text:\n",
      "Sure, here's a breakdown of the macronutrients and their roles in the human body:\n",
      "\n",
      "**Macronutrients:**\n",
      "\n",
      "* **Carbohydrates:**\n",
      "    * Provide energy for the body's cells and tissues.\n",
      "    * Carbohydrates are the primary source of energy for most cells.\n",
      "    * Complex carbohydrates are those that take longer to digest, such as whole grains, fruits, and vegetables.\n",
      "    * Simple carbohydrates are those that are quickly digested, such as sugar, starch, and lactose.\n",
      "\n",
      "* **Proteins:**\n",
      "    * Build and repair tissues, enzymes, and hormones.\n",
      "    * Proteins are essential for immune function, hormone production, and tissue repair.\n",
      "    * There are different types of proteins, each with specific functions.\n",
      "\n",
      "* **Fats:**\n",
      "    * Provide energy, insulation, and help absorb vitamins.\n",
      "    * Healthy fats include olive oil, avocado, nuts, and seeds.\n",
      "    * Trans fats can raise cholesterol levels and increase the risk of heart disease.\n",
      "\n",
      "**Roles of Macronutrients in the Human Body:**\n",
      "\n",
      "* **Energy production:** Carbohydrates, proteins, and fats provide the body with energy.\n",
      "* **Building and repairing tissues:** Proteins are essential for building and repairing tissues, including muscles,\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text: {input_text}\\n\")\n",
    "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
