{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-processing\n",
    "import fitz # install PyMuPDF\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.en import English \n",
    "import re\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "# RAG\n",
    "import torch\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File human-nutrition-text.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# Get PDF document path\n",
    "filename = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF\n",
    "if not os.path.exists(filename):\n",
    "    print(\"File doesn't exist, downloading...\")\n",
    "    \n",
    "    # The URL of the PDF you want to download\n",
    "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "    # Send a GET request to the url\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open the file and save it\n",
    "        with open(filename,\"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"[INFO] The file has been download and saved as {filename}\")\n",
    "    else: print(f\"[INFO] Failed to download the file. Status Code: {response.status_code}\")\n",
    "        \n",
    "else:\n",
    "  print(f\"File {filename} exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format text read from PDF\n",
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\",\" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Provide pdf name\n",
    "def open_and_read_pdf(filename: str)->list[dict]:\n",
    "    doc = fitz.open(filename)\n",
    "    pages_and_texts=[]\n",
    "    for page_number,page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        formatted_text = text_formatter(text)\n",
    "\n",
    "        # store pages information and texts in a dictionary\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                        \"page_char_count\": len(text),\n",
    "                        \"page_word_count\": len(text.split(\" \")),\n",
    "                        \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                        # \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                        \"text\": text})\n",
    "        \n",
    "    return pages_and_texts\n",
    "\n",
    "def read_pages(pages,page_number:int):\n",
    "    # insert pages_and_text and page number to retrieve the specified page in the pdf\n",
    "    content  = pages[page_number+41]\n",
    "    return content\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(pages_and_texts,num_sentence_chunk_size=10,min_token_length=30):\n",
    "    nlp = English()\n",
    "    # Add a sentencizer pipeline\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    # Sentenciaing texts on each page to sentences\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "        \n",
    "        # Make sure all sentences are strings\n",
    "        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "        \n",
    "        # Count the sentences \n",
    "        item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n",
    "    # Loop through pages and texts and split sentences into chunks\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                            slice_size=num_sentence_chunk_size)\n",
    "        item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "\n",
    "    # Split each chunk into its own item\n",
    "    pages_and_chunks = []\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "            \n",
    "            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "            \n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "    df = pd.DataFrame(pages_and_chunks)\n",
    "    pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "    return pages_and_chunks, pages_and_chunks_over_min_token_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(texts,model_name=\"all-mpnet-base-v2\",device=\"cuda\"):\n",
    "\n",
    "    # use \"all-mpnet-base-v2\"\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=model_name, \n",
    "                                        device=device)\n",
    "    # # Send the model to the GPU\n",
    "    # embedding_model.to(\"cuda\") \n",
    "\n",
    "    # Create embeddings one by one on the GPU\n",
    "    # pages_and_chunks_over_min_token_len\n",
    "    for item in tqdm(texts):\n",
    "        item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "\n",
    "    # # Turn text chunks into a single list\n",
    "    # text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "\n",
    "    # # Embed all texts in batches (z-book: 34.4s)\n",
    "    # text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "    #                                                batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "    #                                                convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "    # Save embeddings to file\n",
    "    text_chunks_and_embeddings_df = pd.DataFrame(texts)\n",
    "    embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "    text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)\n",
    "    return text_chunks_and_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_from_file(file='text_chunks_and_embeddings_df.csv',device='cuda'):\n",
    "    # Import texts and embedding df\n",
    "    text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "    # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "    # Convert texts and embedding df to list of dicts\n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "    embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "    return pages_and_chunks,embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [00:00, 1412.77it/s]\n",
      "100%|██████████| 1208/1208 [00:01<00:00, 732.42it/s]\n",
      "100%|██████████| 1208/1208 [00:00<00:00, 384716.72it/s]\n",
      "100%|██████████| 1208/1208 [00:00<00:00, 37960.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'page_number': 196,\n",
       " 'sentence_chunk': 'Potassium also is involved in protein synthesis, energy metabolism, \\nand platelet function, and acts as a buffer in blood, playing a role in \\nacid-base balance.\\nImbalances of Potassium \\nInsufficient potassium levels in the body (hypokalemia) can be \\ncaused by a low dietary intake of potassium or by high sodium \\nintakes, but more commonly it results from medications that \\nincrease water excretion, mainly diuretics. The signs and symptoms \\nof hypokalemia are related to the functions of potassium in nerve \\ncells and consequently skeletal and smooth-muscle contraction.\\nThe signs and symptoms include muscle weakness and cramps, \\nrespiratory distress, and constipation. Severe potassium depletion \\ncan cause the heart to have abnormal contractions and can even \\nbe fatal. High levels of potassium in the blood, or hyperkalemia, \\nalso affects the heart. It is a silent condition as it often displays \\nno signs or symptoms. Extremely high levels of potassium in the \\nblood disrupt the electrical impulses that stimulate the heart and \\ncan cause the heart to stop. Hyperkalemia is usually the result of \\nkidney dysfunction.\\nNeeds and Dietary Sources of Potassium \\nThe IOM based their AIs for potassium on the levels associated with \\na decrease in blood pressure, a reduction in salt sensitivity, and a \\nminimal risk of kidney stones.',\n",
       " 'chunk_char_count': 1332,\n",
       " 'chunk_word_count': 203,\n",
       " 'chunk_token_count': 333.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing\n",
    "pages_and_texts = open_and_read_pdf(\"human-nutrition-text.pdf\")\n",
    "pages_and_chunks,pages_and_chunks_over_min_token_len = text_processor(pages_and_texts)\n",
    "read_pages(pages_and_chunks_over_min_token_len,258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1683/1683 [00:39<00:00, 42.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Try embedding function\n",
    "text_chunks_and_embeddings_df = embedding(pages_and_chunks_over_min_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embedding from csv file\n",
    "pages_and_chunks,embeddings = read_embedding_from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG - Search and Answer\n",
    "\n",
    "We discussed RAG briefly in the beginning but let's quickly recap.\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n",
    "\n",
    "Let's breakdown each step:\n",
    "* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n",
    "* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n",
    "* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n",
    "\n",
    "The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n",
    "\n",
    "This is an incredibly helpful tool.\n",
    "\n",
    "Let's say you had 1000s of customer support documents.\n",
    "\n",
    "You could use RAG to generate direct answers to questions with links to relevant documentation.\n",
    "\n",
    "Or you were an insurance company with large chains of claims emails.\n",
    "\n",
    "You could use RAG to answer questions about the emails with sources.\n",
    "\n",
    "One helpful analogy is to think of LLMs as calculators for words.\n",
    "\n",
    "With good inputs, the LLM can sort them into helpful outputs.\n",
    "\n",
    "How? \n",
    "\n",
    "It starts with better search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search\n",
    "\n",
    "Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n",
    "\n",
    "If this sounds like woo, woo. It's not.\n",
    "\n",
    "Perhaps searching via *meaning* is a better analogy.\n",
    "\n",
    "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
    "\n",
    "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
    "\n",
    "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n",
    "\n",
    "> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with: \n",
    ">\n",
    ">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n",
    "> \n",
    "> as the first result. How cool!\n",
    "\n",
    "If you've ever used Google, you know this kind of workflow.\n",
    "\n",
    "But now we'd like to perform that across our own data.\n",
    "\n",
    "Let's import our embeddings we created earlier (tk -link to embedding file) and prepare them for use by turning them into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are ready!\n",
    "\n",
    "Time to perform a semantic search.\n",
    "\n",
    "Let's say you were studying the macronutrients.\n",
    "\n",
    "And wanted to search your textbook for \"macronutrients functions\".\n",
    "\n",
    "Well, we can do so with the following steps:\n",
    "1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n",
    "2. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
    "3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n",
    "4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: macronutrients functions\n",
      "Time take to get scores on 1683 embeddings: 0.00180 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6926, 0.6738, 0.6646, 0.6536, 0.6473], device='cuda:0'),\n",
       "indices=tensor([42, 47, 41, 51, 46], device='cuda:0'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the query\n",
    "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
    "query = \"macronutrients functions\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples \n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~0.0018 seconds to perform a dot product comparison across 1680 embeddings on my machine\n",
    "\n",
    "So even if you we're to increase our embeddings by 100x (1680 -> 168,000), an exhaustive dot product operation would happen in ~0.18 seconds (assuming linear scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we can get pretty far by just storing our embeddings in `torch.tensor` for now.\n",
    "\n",
    "However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "Let's check the results of our original similarity search.\n",
    "\n",
    "[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n",
    "\n",
    "The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n",
    "\n",
    "We can use those indicies to map back to our text chunks.\n",
    "\n",
    "We already defined a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line).\n",
    "\n",
    "Then we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6926, 0.6738, 0.6646, 0.6536, 0.6473], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results_dot_product[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42, device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [1] is the score array\n",
    "# [0] is the first element in the score array\n",
    "top_results_dot_product[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macronutrients  Nutrients  that  are  needed  in  large  amounts  are  called\n",
      "macronutrients. There are three classes of macronutrients:  carbohydrates,\n",
      "lipids, and proteins. These can be metabolically  processed into cellular\n",
      "energy. The energy from macronutrients  comes from their chemical bonds. This\n",
      "chemical energy is  converted into cellular energy that is then utilized to\n",
      "perform work,  allowing our bodies to conduct their basic functions. A unit of\n",
      "measurement of food energy is the calorie. On nutrition food labels  the amount\n",
      "given for “calories” is actually equivalent to each calorie  multiplied by one\n",
      "thousand. A kilocalorie (one thousand calories,  denoted with a small “c”) is\n",
      "synonymous with the “Calorie” (with a  capital “C”) on nutrition food labels.\n",
      "Water is also a macronutrient in  the sense that you require a large amount of\n",
      "it, but unlike the other  macronutrients, it does not yield calories.\n",
      "Carbohydrates  Carbohydrates are molecules composed of carbon, hydrogen, and\n",
      "oxygen.\n"
     ]
    }
   ],
   "source": [
    "print_wrapped(pages_and_chunks[top_results_dot_product[1][0]]['sentence_chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'macronutrients functions'\n",
      "\n",
      "Results:\n",
      "Score: 0.6926\n",
      "Text:\n",
      "Macronutrients  Nutrients  that  are  needed  in  large  amounts  are  called\n",
      "macronutrients. There are three classes of macronutrients:  carbohydrates,\n",
      "lipids, and proteins. These can be metabolically  processed into cellular\n",
      "energy. The energy from macronutrients  comes from their chemical bonds. This\n",
      "chemical energy is  converted into cellular energy that is then utilized to\n",
      "perform work,  allowing our bodies to conduct their basic functions. A unit of\n",
      "measurement of food energy is the calorie. On nutrition food labels  the amount\n",
      "given for “calories” is actually equivalent to each calorie  multiplied by one\n",
      "thousand. A kilocalorie (one thousand calories,  denoted with a small “c”) is\n",
      "synonymous with the “Calorie” (with a  capital “C”) on nutrition food labels.\n",
      "Water is also a macronutrient in  the sense that you require a large amount of\n",
      "it, but unlike the other  macronutrients, it does not yield calories.\n",
      "Carbohydrates  Carbohydrates are molecules composed of carbon, hydrogen, and\n",
      "oxygen.\n",
      "Page number: 5\n",
      "\n",
      "\n",
      "Score: 0.6738\n",
      "Text:\n",
      "Water  There is one other nutrient that we must have in large quantities:\n",
      "water. Water does not contain carbon, but is composed of two  hydrogens and one\n",
      "oxygen per molecule of water. More than 60  percent of your total body weight is\n",
      "water. Without it, nothing could  be transported in or out of the body, chemical\n",
      "reactions would not  occur, organs would not be cushioned, and body temperature\n",
      "would  fluctuate widely. On average, an adult consumes just over two liters  of\n",
      "water per day from food and drink combined. Since water is so  critical for\n",
      "life’s basic processes, the amount of water input and  output is supremely\n",
      "important, a topic we will explore in detail in  Chapter 4. Micronutrients\n",
      "Micronutrients are nutrients required by the body in lesser  amounts, but are\n",
      "still essential for carrying out bodily functions. Micronutrients include all\n",
      "the essential minerals and vitamins. There are sixteen essential minerals and\n",
      "thirteen vitamins (See  Table 1.1 “Minerals and Their Major Functions” and Table\n",
      "1.2  “Vitamins and Their Major Functions” for a complete list and their  major\n",
      "functions). In contrast to carbohydrates, lipids, and proteins,  micronutrients\n",
      "are not sources of energy (calories), but they assist  in the process as\n",
      "cofactors or components of enzymes (i.e.,  coenzymes).\n",
      "Page number: 8\n",
      "\n",
      "\n",
      "Score: 0.6646\n",
      "Text:\n",
      "Learning Objectives  By the end of this chapter, you will be able to:  •\n",
      "Describe basic concepts in nutrition  •  Describe factors that affect your\n",
      "nutritional needs  •  Describe the importance of research and scientific\n",
      "methods to understanding nutrition  What are Nutrients? The foods we eat contain\n",
      "nutrients. Nutrients are substances  required by the body to perform its basic\n",
      "functions. Nutrients must  be obtained from our diet, since the human body does\n",
      "not  synthesize or produce them. Nutrients have one or more of three  basic\n",
      "functions: they provide energy, contribute to body structure,  and/or regulate\n",
      "chemical processes in the body. These basic  functions allow us to detect and\n",
      "respond to environmental  surroundings, move, excrete wastes, respire (breathe),\n",
      "grow, and  reproduce. There are six classes of nutrients required for the body\n",
      "to function and maintain overall health. These are carbohydrates,  lipids,\n",
      "proteins, water, vitamins, and minerals. Foods also contain  non-nutrients that\n",
      "may be harmful (such as natural toxins common  in plant foods and additives like\n",
      "some dyes and preservatives) or  beneficial (such as antioxidants). 4 |\n",
      "Introduction\n",
      "Page number: 4\n",
      "\n",
      "\n",
      "Score: 0.6536\n",
      "Text:\n",
      "Vitamins  Major Functions  Water-soluble  Thiamin (B1)  Coenzyme, energy\n",
      "metabolism assistance  Riboflavin (B2 )  Coenzyme, energy metabolism assistance\n",
      "Niacin (B3)  Coenzyme, energy metabolism assistance  Pantothenic acid  (B5)\n",
      "Coenzyme, energy metabolism assistance  Pyridoxine (B6)  Coenzyme, amino acid\n",
      "synthesis assistance  Biotin (B7)  Coenzyme, amino acid and fatty acid\n",
      "metabolism  Folate (B9)  Coenzyme, essential for growth  Cobalamin (B12)\n",
      "Coenzyme, red blood cell synthesis  C (ascorbic acid)  Collagen synthesis,\n",
      "antioxidant  Fat-soluble  A  Vision, reproduction, immune system function  D\n",
      "Bone and teeth health maintenance, immune system  function  E  Antioxidant, cell\n",
      "membrane protection  K  Bone and teeth health maintenance, blood clotting\n",
      "Vitamin deficiencies can cause severe health problems and even  death. For\n",
      "example, a deficiency in niacin causes a disease called  pellagra, which was\n",
      "common in the early twentieth century in some  parts of America. The common\n",
      "signs and symptoms of pellagra  are known as the “4D’s—diarrhea, dermatitis,\n",
      "dementia, and death.” Until scientists found out that better diets relieved the\n",
      "signs and  symptoms of pellagra, many people with the disease ended up\n",
      "hospitalized in insane asylums awaiting death. Other vitamins were  also found\n",
      "to prevent certain disorders and diseases such as scurvy  (vitamin C), night\n",
      "blindness vitamin A, and rickets (vitamin D). Table 1.3 Functions of Nutrients\n",
      "Introduction | 11\n",
      "Page number: 11\n",
      "\n",
      "\n",
      "Score: 0.6473\n",
      "Text:\n",
      "Figure 1.1 The  Macronutrie nts:  Carbohydrat es, Lipids,  Protein, and  Water\n",
      "Proteins  Proteins are macromolecules composed of chains of subunits called\n",
      "amino acids. Amino acids are simple subunits composed of carbon,  oxygen,\n",
      "hydrogen, and nitrogen. Food sources of proteins include  meats, dairy products,\n",
      "seafood, and a variety of different plant- based foods, most notably soy. The\n",
      "word protein comes from a  Greek word meaning “of primary importance,” which is\n",
      "an apt  description of these macronutrients; they are also known  colloquially\n",
      "as the “workhorses” of life. Proteins provide four  kilocalories of energy per\n",
      "gram; however providing energy is not  protein’s most important function.\n",
      "Proteins provide structure to  bones, muscles and skin, and play a role in\n",
      "conducting most of the  chemical reactions that take place in the body.\n",
      "Scientists estimate  that greater than one-hundred thousand different proteins\n",
      "exist  within the human body. The genetic codes in DNA are basically  protein\n",
      "recipes that determine the order in which 20 different  amino acids are bound\n",
      "together to make thousands of specific  proteins. Figure 1.1 The Macronutrients:\n",
      "Carbohydrates, Lipids, Protein, and  Water  Introduction | 7\n",
      "Page number: 7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print more info from together with the result\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
